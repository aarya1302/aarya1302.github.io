<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Strategies for "The Resistance" Game</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
            color: #333;
        }
        .container {
            max-width: 800px;
            margin: 20px auto;
            background: #fff;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
        }
        h1, h2, h3 {
            color: #0056b3;
        }
        a {
            color: #0056b3;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>AI Strategies for "The Resistance" Game</h1>

        <h2>Problem Definition</h2>
        <p>The primary challenge in "The Resistance" game is identifying who the spies are and who the loyal players (resistance members) are.</p>

        <h3>Key Elements:</h3>
        <ul>
            <li><strong>State:</strong> The state of the game includes mission history, the players in the game, and the beliefs of the agent.</li>
            <li><strong>Actions:</strong> Agents can propose a mission, vote on a mission, and betray a mission if they are spies.</li>
            <li><strong>Outcomes:</strong> A mission team can be accepted or rejected, and missions can pass or fail based on the actions of agents.</li>
            <li><strong>Goals:</strong> Resistance agents aim to maximize successful missions, while spies aim to sabotage missions without being detected.</li>
        </ul>

        <h2>Methodologies</h2>

        <h3>1. MiniMax Agent</h3>
        <p>The MiniMax algorithm, typically used for zero-sum games, can be adapted to generate a decision tree for this game. Nodes in the tree reflect mission proposals, voting outcomes, and mission success or failure. The agent selects actions that maximize its team's success:</p>
        <ul>
            <li><strong>Resistance:</strong> Minimize the chance of spies infiltrating missions.</li>
            <li><strong>Spies:</strong> Maximize the likelihood of sabotaging missions without revealing their identity.</li>
        </ul>
        <p><em>Limitations:</em> This method is computationally expensive due to high space and time complexity in non-zero-sum multiplayer games.</p>

        <h3>2. Reinforcement Q-Learning Agent</h3>
        <p>A reinforcement learning agent uses rewards to update its strategy after each round. Actions include voting "Yes" for a mission, proposing teams, or betraying missions. The agent adjusts Q-values based on mission success or failure:</p>
        <ul>
            <li>If a team fails, Q-values for voting "Yes" or proposing similar teams decrease.</li>
            <li>For spies, rewards are based on the ratio of spies to team members when sabotaging a mission.</li>
        </ul>
        <p><em>Challenges:</em> With only five rounds, it is difficult to assign meaningful Q-values. The exploration rate must be set very low, limiting learning opportunities.</p>

        <h3>3. Hidden Markov Model (HMM) Agent</h3>
        <p>An HMM agent uses probabilistic reasoning to identify spies by observing player behavior:</p>
        <ul>
            <li><strong>Hidden Variable:</strong> Whether a player is a spy or not.</li>
            <li><strong>Observations:</strong> Player participation in failed teams, votes for failed teams, and proposals of failed teams.</li>
        </ul>
        <p>The agent starts with an initial belief (1/3 probability of being a spy) and updates trust levels based on mission outcomes and voting behaviors:</p>
        <ul>
            <li>Trust increases for players on successful missions.</li>
            <li>Trust decreases for players involved in failed missions.</li>
        </ul>

        <h2>Performance</h2>
        <p>The HMM agent demonstrated the highest win rate across 10 tournaments, reliably outperforming other strategies.</p>

        <h2>Comparison of Different Techniques</h2>

        <h3>Merits of the Reinforcement Learning Agent</h3>
        <ul>
            <li>Performs well in generalized situations with a simple model.</li>
            <li>Not significantly affected by variables like agent type or player count.</li>
        </ul>

        <h3>Merits of the HMM Agent</h3>
        <ul>
            <li>Creates a reliable prediction model over five rounds.</li>
            <li>Adapts strategies based on observed behavior.</li>
        </ul>

        <h3>Challenges for Reinforcement Learning</h3>
        <ul>
            <li>Struggles with limited data in only five rounds.</li>
            <li>Fails to account for voting history, leading to inaccurate Q-values.</li>
            <li>Does not maintain memory across games in a tournament, limiting learning opportunities.</li>
        </ul>

        <h3>Why HMM Agent Outperforms RL Agent</h3>
        <p>The Bayesian (HMM) agent updates trust levels effectively, even with limited rounds. It accurately identifies spies by considering key factors like voting patterns and mission outcomes. Reinforcement learning, on the other hand, falters due to limited data and the inability to learn across games.</p>

        <h2>Conclusion</h2>
        <p>The Bayesian (HMM) model is ideal for "The Resistance" because it efficiently handles uncertainty, updates beliefs in real-time, and adapts to the game's dynamics. Its superior performance in identifying spies makes it the best choice among the evaluated techniques, outperforming both reinforcement learning and MiniMax strategies.</p>

        <p>Have thoughts or feedback? Share them below or connect with me to discuss AI strategies for complex games!</p>
    </div>
</body>
</html>

