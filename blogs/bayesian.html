<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Strategies for "The Resistance" Game</title>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        font-family: "Inter", sans-serif;
      }

      .navbar {
        position: fixed;
        top: 0;
        width: 100%;
        padding: 1.5rem;
        background-color: rgba(10, 10, 10, 0.95);
        backdrop-filter: blur(10px);
        z-index: 1000;
        border-bottom: 1px solid rgba(255, 255, 255, 0.1);
      }

      .nav-links {
        display: flex;
        justify-content: center;
        gap: 2rem;
      }

      .nav-links a {
        color: #ffffff;
        text-decoration: none;
        font-size: 1.1rem;
        transition: color 0.3s;
        font-weight: 500;
      }

      .nav-links a:hover {
        color: #64ffda;
      }

      body {
        background-color: #0a0a0a;
        color: #e0e0e0;
        line-height: 1.6;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 5rem;
      }

      .project-header {
        text-align: center;
        margin-bottom: 2rem;
      }

      .project-header h1 {
        font-size: 3rem;
        color: #64ffda;
      }

      .project-header p {
        font-size: 1.2rem;
        color: #ffffff;
      }

      .section {
        margin-bottom: 2rem;
      }

      .section-title {
        font-size: 2rem;
        margin-bottom: 1rem;
        color: #64ffda;
        border-bottom: 2px solid #64ffda;
        display: inline-block;
      }

      .section-content {
        margin-top: 1rem;
      }

      .section-content p {
        margin-bottom: 1rem;
        color: #ffffff;
      }

      .technologies {
        display: flex;
        flex-wrap: wrap;
        gap: 1rem;
      }

      .technology-item {
        background: rgba(255, 255, 255, 0.1);
        padding: 0.5rem 1rem;
        border-radius: 5px;
        color: #ffffff;
        font-weight: 500;
      }
    </style>
  </head>
  <body>
    <nav class="navbar">
      <div class="nav-links">
        <a href="https://aarya1302.github.io/#hero">Home</a>
        <a href="https://aarya1302.github.io/#about">About</a>
        <a href="https://aarya1302.github.io/#experience">Experience</a>
        <a href="https://aarya1302.github.io/#projects">Projects</a>
        <a href="https://aarya1302.github.io/#contact">Contact</a>
      </div>
    </nav>

    <div class="container">
      <header class="project-header">
        <h1>AI Strategies for "The Resistance" Game</h1>
        <p>
          An exploration of AI techniques to master the social deduction game
          "The Resistance."
        </p>
      </header>

      <section id="overview" class="section">
        <h2 class="section-title">Overview</h2>
        <div class="section-content">
          <p>
            "The Resistance" is a social deduction game where players take on
            the roles of loyal resistance members or spies working against them.
            This project involves creating AI agents that can analyze, adapt,
            and thrive in the dynamic and unpredictable environment of the game.
          </p>
        </div>
      </section>

      <section id="strategies" class="section">
        <h2 class="section-title">Problem Definition</h2>
        <div class="section-content">
          <p>
            <strong>State: </strong>The state of the game can be described by
            mission history, the players in the games and beliefs of the agent.
          </p>
          <p>
            <strong>Actions: </strong> Agent would be able to do three actions
            propose a mission, vote on a mission and betray a mission if they’re
            a spy
          </p>
          <p>
            <strong>Outcomes: </strong> Based on the agent’s actions a mission
            team can be rejected or accepted, and mission can pass or fail.
          </p>
          <p>
            <strong>Goals: </strong> For an agent who is part of the resistance
            the goal is to maximize the number of passed mission and minimize
            the number of failed mission, and if agent is spy the goal is to do
            the opposite.
          </p>
        </div>
      </section>

      <section id="ai-design" class="section">
        <h2 class="section-title">Methodologies</h2>
        <div class="section-content">
          <h1>
            Agents for The Resistance: MiniMax, Reinforcement Q-Learning, and
            Hidden Markov Model
          </h1>

          <h2>MiniMax Agent</h2>
          <p>
            While the MiniMax algorithm is typically used for zero-sum games, it
            can be employed here to generate a game state as a decision tree
            where nodes reflect mission proposals, voting outcomes, and mission
            success or failure. The agent aims to maximize its team’s chances of
            success:
          </p>
          <ul>
            <li>
              For the resistance team, it minimizes the chance of spies
              infiltrating missions.
            </li>
            <li>
              For the spy team, it maximizes the likelihood of sabotaging
              missions without revealing their identity.
            </li>
          </ul>
          <p>
            However, since this is not a zero-sum game with only two players,
            this approach results in very high space and time complexity.
          </p>

          <h2>Reinforcement Q-Learning Agent</h2>
          <p>
            A Reinforcement Learning (RL) agent tracks rewards for its actions
            after each round. For example:
          </p>
          <ul>
            <li>
              <strong>Voting "Yes" for a proposed team:</strong> If the team
              fails, the Q-value for voting "Yes" for teams containing the same
              players decreases. Similarly, the Q-value for proposing such teams
              also decreases.
            </li>
            <li>Conversely, if the mission succeeds, the Q-values increase.</li>
          </ul>
          <p>
            If the agent is a spy, it tracks when missions fail, maintaining an
            internal state of the team size, number of betrayals causing
            failure, and number of spies in the team. It rewards missions with a
            favorable ratio of spies to team members for voting to betray.
          </p>

          <h2>Hidden Markov Model (HMM) Agent</h2>
          <p>
            The HMM agent is well-suited for identifying spies, leveraging
            probabilistic reasoning to make decisions based on player behavior.
          </p>
          <ul>
            <li>
              <strong>Hidden Variable:</strong> Whether a player is a spy or
              not.
            </li>
            <li>
              <strong>Observations:</strong> Key observations include:
              <ul>
                <li>The player is part of a failed team.</li>
                <li>The player voted for a failed team.</li>
                <li>The player proposed a failed team.</li>
              </ul>
            </li>
          </ul>
          <p>
            <strong>Initial Belief:</strong> The agent starts with a 1/3
            probability that any player is a spy.
          </p>
          <p>
            <strong>Belief Updating:</strong> Trust increases for players part
            of successful missions and decreases for those involved in failed
            missions or who voted for a failing team.
          </p>
          <p>
            <strong>Actions:</strong>
          </p>
          <ul>
            <li>
              Votes for or against proposed teams based on trust levels of the
              players involved.
            </li>
            <li>
              Proposes teams prioritizing players with the highest trust levels.
            </li>
            <li>
              If the agent is a spy, it betrays only when the required number of
              betrayals matches to ensure the mission’s failure.
            </li>
          </ul>

          <h2>Performance</h2>
          <p>
            The HMM agent outperforms reference agents across tournaments,
            achieving the highest win-rate across 10 tournaments.
          </p>
        </div>
      </section>

      <section id="technologies" class="section">
        <h2 class="section-title">Technologies Used</h2>
        <div class="technologies">
          <div class="technology-item">Python</div>
          <div class="technology-item">Machine Learning</div>
          <div class="technology-item">Bayesian Networks</div>
          <div class="technology-item">Heuristic Algorithms</div>
          <div class="technology-item">Game Theory</div>
        </div>
      </section>

      <section id="conclusion" class="section">
        <h2 class="section-title">Conclusion</h2>
        <div class="section-content">
          <p>
            This project highlights the potential of AI in social deduction
            games. By leveraging advanced strategies and technologies, the AI
            agents demonstrate the ability to adapt and excel, providing
            valuable insights into human behavior and decision-making.
          </p>
        </div>
      </section>
    </div>
  </body>
</html>
