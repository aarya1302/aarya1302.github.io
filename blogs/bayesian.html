<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Strategies for "The Resistance" Game</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        font-family: "Inter", sans-serif;
      }

      body {
        background-color: #0a0a0a;
        color: #e0e0e0;
        line-height: 1.6;
      }

      .navbar {
        position: fixed;
        top: 0;
        width: 100%;
        padding: 1.5rem;
        background-color: rgba(10, 10, 10, 0.95);
        backdrop-filter: blur(10px);
        z-index: 1000;
        border-bottom: 1px solid rgba(255, 255, 255, 0.1);
      }

      .nav-links {
        display: flex;
        justify-content: center;
        gap: 2rem;
      }

      .nav-links a {
        color: #ffffff;
        text-decoration: none;
        font-size: 1.1rem;
        transition: color 0.3s;
        font-weight: 500;
      }

      .nav-links a:hover {
        color: #64ffda;
      }

      section {
        min-height: 100vh;
        padding: 6rem 2rem 2rem;
        display: flex;
        flex-direction: column;
        justify-content: center;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
      }

      #hero {
        background: linear-gradient(45deg, #0a0a0a, #1a1a1a);
      }

      .hero-content {
        opacity: 1;
        transform: translateY(30px);
      }

      .hero-content h1 {
        font-size: 4rem;
        margin-bottom: 1rem;
        color: #ffffff;
        text-shadow: 0 0 10px rgba(100, 255, 218, 0.3);
      }

      .hero-content p {
        font-size: 1.5rem;
        color: #64ffda;
        font-weight: 300;
      }

      .section-title {
        font-size: 2.5rem;
        margin-bottom: 2rem;
        color: #ffffff;
        border-bottom: 2px solid #64ffda;
        display: inline-block;
        padding-bottom: 0.5rem;
      }

      .experience-item,
      .project-card {
        background: rgba(255, 255, 255, 0.1);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        opacity: 1;
        transform: translateY(30px);
        border: 1px solid rgba(255, 255, 255, 0.1);
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      }

      .experience-item h3,
      .project-card h3 {
        color: #64ffda;
        margin-bottom: 1rem;
        font-size: 1.5rem;
      }

      .experience-item p,
      .project-card p {
        color: #ffffff;
        font-size: 1.1rem;
        margin-bottom: 0.5rem;
      }

      .experience-item .date {
        color: #64ffda;
        font-size: 0.9rem;
        margin-bottom: 1rem;
      }

      .project-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        gap: 2rem;
      }

      .contact-form {
        display: flex;
        flex-direction: column;
        gap: 1rem;
        max-width: 600px;
        margin: 0 auto;
        opacity: 1;
        transform: translateY(30px);
      }

      .contact-form input,
      .contact-form textarea {
        padding: 1rem;
        background: rgba(255, 255, 255, 0.1);
        border: 1px solid rgba(255, 255, 255, 0.2);
        border-radius: 5px;
        color: #ffffff;
        font-size: 1rem;
      }

      .contact-form input::placeholder,
      .contact-form textarea::placeholder {
        color: rgba(255, 255, 255, 0.6);
      }

      .contact-form button {
        padding: 1rem 2rem;
        background: #64ffda;
        color: #0a0a0a;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        transition: all 0.3s;
        font-weight: bold;
        font-size: 1.1rem;
      }

      .contact-form button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(100, 255, 218, 0.3);
      }

      @media (max-width: 768px) {
        .hero-content h1 {
          font-size: 3rem;
        }

        .nav-links {
          gap: 1rem;
        }

        .section-title {
          font-size: 2rem;
        }
      }
    </style>
    >
  </head>
  <body>
    <nav class="navbar">
      <div class="nav-links">
        <a href="aarya1302.github.io/#hero">Home</a>
        <a href="aarya1302.github.io/#about">About</a>
        <a href="aarya1302.github.io/#experience">Experience</a>
        <a href="aarya1302.github.io/#projects">Projects</a>
        <a href="aarya1302.github.io/#contact">Contact</a>
      </div>
    </nav>

    <div class="container">
      <h1>AI Strategies for "The Resistance" Game</h1>

      <h2>Problem Definition</h2>
      <p>
        The primary challenge in "The Resistance" game is identifying who the
        spies are and who the loyal players (resistance members) are.
      </p>

      <h3>Key Elements:</h3>
      <ul>
        <li>
          <strong>State:</strong> The state of the game includes mission
          history, the players in the game, and the beliefs of the agent.
        </li>
        <li>
          <strong>Actions:</strong> Agents can propose a mission, vote on a
          mission, and betray a mission if they are spies.
        </li>
        <li>
          <strong>Outcomes:</strong> A mission team can be accepted or rejected,
          and missions can pass or fail based on the actions of agents.
        </li>
        <li>
          <strong>Goals:</strong> Resistance agents aim to maximize successful
          missions, while spies aim to sabotage missions without being detected.
        </li>
      </ul>

      <h2>Methodologies</h2>

      <h3>1. MiniMax Agent</h3>
      <p>
        The MiniMax algorithm, typically used for zero-sum games, can be adapted
        to generate a decision tree for this game. Nodes in the tree reflect
        mission proposals, voting outcomes, and mission success or failure. The
        agent selects actions that maximize its team's success:
      </p>
      <ul>
        <li>
          <strong>Resistance:</strong> Minimize the chance of spies infiltrating
          missions.
        </li>
        <li>
          <strong>Spies:</strong> Maximize the likelihood of sabotaging missions
          without revealing their identity.
        </li>
      </ul>
      <p>
        <em>Limitations:</em> This method is computationally expensive due to
        high space and time complexity in non-zero-sum multiplayer games.
      </p>

      <h3>2. Reinforcement Q-Learning Agent</h3>
      <p>
        A reinforcement learning agent uses rewards to update its strategy after
        each round. Actions include voting "Yes" for a mission, proposing teams,
        or betraying missions. The agent adjusts Q-values based on mission
        success or failure:
      </p>
      <ul>
        <li>
          If a team fails, Q-values for voting "Yes" or proposing similar teams
          decrease.
        </li>
        <li>
          For spies, rewards are based on the ratio of spies to team members
          when sabotaging a mission.
        </li>
      </ul>
      <p>
        <em>Challenges:</em> With only five rounds, it is difficult to assign
        meaningful Q-values. The exploration rate must be set very low, limiting
        learning opportunities.
      </p>

      <h3>3. Hidden Markov Model (HMM) Agent</h3>
      <p>
        An HMM agent uses probabilistic reasoning to identify spies by observing
        player behavior:
      </p>
      <ul>
        <li>
          <strong>Hidden Variable:</strong> Whether a player is a spy or not.
        </li>
        <li>
          <strong>Observations:</strong> Player participation in failed teams,
          votes for failed teams, and proposals of failed teams.
        </li>
      </ul>
      <p>
        The agent starts with an initial belief (1/3 probability of being a spy)
        and updates trust levels based on mission outcomes and voting behaviors:
      </p>
      <ul>
        <li>Trust increases for players on successful missions.</li>
        <li>Trust decreases for players involved in failed missions.</li>
      </ul>

      <h2>Performance</h2>
      <p>
        The HMM agent demonstrated the highest win rate across 10 tournaments,
        reliably outperforming other strategies.
      </p>

      <h2>Comparison of Different Techniques</h2>

      <h3>Merits of the Reinforcement Learning Agent</h3>
      <ul>
        <li>Performs well in generalized situations with a simple model.</li>
        <li>
          Not significantly affected by variables like agent type or player
          count.
        </li>
      </ul>

      <h3>Merits of the HMM Agent</h3>
      <ul>
        <li>Creates a reliable prediction model over five rounds.</li>
        <li>Adapts strategies based on observed behavior.</li>
      </ul>

      <h3>Challenges for Reinforcement Learning</h3>
      <ul>
        <li>Struggles with limited data in only five rounds.</li>
        <li>
          Fails to account for voting history, leading to inaccurate Q-values.
        </li>
        <li>
          Does not maintain memory across games in a tournament, limiting
          learning opportunities.
        </li>
      </ul>

      <h3>Why HMM Agent Outperforms RL Agent</h3>
      <p>
        The Bayesian (HMM) agent updates trust levels effectively, even with
        limited rounds. It accurately identifies spies by considering key
        factors like voting patterns and mission outcomes. Reinforcement
        learning, on the other hand, falters due to limited data and the
        inability to learn across games.
      </p>

      <h2>Conclusion</h2>
      <p>
        The Bayesian (HMM) model is ideal for "The Resistance" because it
        efficiently handles uncertainty, updates beliefs in real-time, and
        adapts to the game's dynamics. Its superior performance in identifying
        spies makes it the best choice among the evaluated techniques,
        outperforming both reinforcement learning and MiniMax strategies.
      </p>

      <p>
        Have thoughts or feedback? Share them below or connect with me to
        discuss AI strategies for complex games!
      </p>
    </div>
  </body>
</html>
